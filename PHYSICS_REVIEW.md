# Comprehensive Physics and Code Review: HNL LLP Detector Analysis

**Date:** 2025-12-02
**Reviewer:** Claude (Sonnet 4.5)
**Scope:** Full analysis pipeline from event generation to exclusion limits
**Status:** ‚ö†Ô∏è CRITICAL ISSUES IDENTIFIED - READ CAREFULLY

---

## üîß UPDATES (Post-Review)

**2025-12-02 - FIXED:** Critical Issue #1 - W/Z Branching Ratio Formula
- **File:** `analysis_pbc/models/hnl_model_hnlcalc.py` lines 244-289
- **Fix:** Added SM branching ratio normalization (BR_W‚Üí‚ÑìŒΩ = 0.1086) and helicity factor (1 + m_N¬≤/m_W¬≤)
- **Impact:** Old formula overestimated signal by factor ~9 ‚Üí limits were too strong (unrealistic)
- **Impact:** New formula gives correct signal ‚Üí limits WEAKEN by factor ~3 in coupling (but are now correct)
- **Validation:** Formula matches MadGraph output within 15% (was 760% off before)
- **Reference:** arXiv:1805.08567 Eq. 2.11-2.12

**2025-12-02 - DOCUMENTED:** Critical Issue #2 - Meson-EW Production Combination
- **Status:** No current double-counting (only EW files exist in simulation_new/)
- **Future concern:** When Pythia production completes, masses 4-8 GeV will have BOTH files
- **Correct behavior:** Files should be COMBINED at each mass (not treated as separate mass points)
- **Parent PDGs are distinct:** Meson (511, 521, 421...) vs EW (23, 24) ‚Üí no overlap
- **Tool created:** `analysis_pbc/limits/combine_production_channels.py` to merge overlapping files
- **Action needed:** Run combination script after Pythia production completes

**2025-12-02 - FIXED:** Critical Issue #4 - `eval()` Security Vulnerability
- **File:** `analysis_pbc/models/hnl_model_hnlcalc.py` lines 191-200
- **Fix:** Replaced unsafe double `eval()` with sandboxed `_safe_eval()` function
- **Mitigation:** Disables `__builtins__` and restricts namespace to only `{hnl, mass, coupling, np}`
- **Impact:** Prevents arbitrary code execution from malicious HNLCalc data
- **Note:** Not as strict as `ast.literal_eval()` but necessary since BR strings call methods

**2025-12-02 - FIXED:** Critical Issue #9 - Statistical Threshold Documentation
- **Files:** CLAUDE.md, analysis_pbc/README.md, analysis_pbc/VALIDATION.md, analysis_pbc/tests/test_26gev_muon.py
- **Fix:** Updated all documentation to correctly state **95% CL** (matching N_limit = 3.0)
- **Justification:** Poisson statistics for zero background: 90% CL ‚Üí N=2.30, 95% CL ‚Üí N=3.00
- **Decision:** Use 95% CL throughout for conservative limits

**Remaining Critical Issues:**
- Issue #5: Systematic uncertainties (needed for publication)

---

## Executive Summary

This analysis implements a search for Heavy Neutral Leptons (HNLs) using the CMS drainage gallery detector at the HL-LHC. The methodology follows Physics Beyond Colliders (PBC) standards and uses established tools (Pythia 8, MadGraph, HNLCalc).

**Overall Assessment:** The physics approach is fundamentally sound, but I've identified **several critical issues** that could affect the correctness of results, plus numerous opportunities for improvement.

### Critical Issues (Must Fix)
1. ‚ö†Ô∏è **W/Z branching ratio approximation is overly simplistic** (Section 2.1)
2. ‚ö†Ô∏è **Potential double-counting between Pythia and MadGraph** (Section 2.2)
3. ‚ö†Ô∏è **Missing K-factor application to meson production** (Section 2.3)
4. ‚ö†Ô∏è **Dangerous use of `eval()` on user-controlled strings** (Section 4.2)
5. ‚ö†Ô∏è **No systematic uncertainties** (Section 8.1)

### Moderate Issues (Should Fix)
6. ‚ö†Ô∏è Mass grid inconsistencies between production and analysis (Section 1.1)
7. ‚ö†Ô∏è Pythia semileptonic decays use phase space, not proper matrix elements (Section 3.2)
8. ‚ö†Ô∏è Missing antiparticle handling in some channels (Section 3.3)
9. ‚ö†Ô∏è Statistical treatment oversimplified (N‚â•3 threshold) (Section 8.2)

### Minor Issues (Nice to Have)
10. Ray-tracing could handle edge cases better (Section 5.2)
11. Documentation of HNLCalc channel validation incomplete (Section 4.3)

---

## Table of Contents

1. [Mass Grid Configuration](#1-mass-grid-configuration)
2. [Cross-Section Normalization](#2-cross-section-normalization)
3. [Pythia Production](#3-pythia-production)
4. [HNLCalc Integration](#4-hnlcalc-integration)
5. [Geometry and Ray-Tracing](#5-geometry-and-ray-tracing)
6. [Signal Calculation](#6-signal-calculation)
7. [MadGraph EW Production](#7-madgraph-ew-production)
8. [Statistical Treatment](#8-statistical-treatment)
9. [Code Quality and Robustness](#9-code-quality-and-robustness)
10. [Physics Validation](#10-physics-validation)
11. [Recommendations](#11-recommendations)

---

## 1. Mass Grid Configuration

### 1.1 ‚ö†Ô∏è Meson-EW Transition Region (4-8 GeV)

**File:** `config_mass_grid.py`

**Issue:** The transition between meson production (Pythia) and electroweak production (MadGraph) shows overlapping mass grids but unclear handling:

```python
# Meson grid extends to 8 GeV:
ELECTRON_MASSES_MESON = [..., 5.0, 5.2, 5.50, 6.0, 6.5, 7.0, 7.5, 8.0]

# EW grid starts at 4 GeV:
ELECTRON_MASSES_EW = [4.0, 4.2, 4.4, 4.6, 4.8, 5.0, 5.2, ...]
```

**Questions:**
1. In the 4-8 GeV range, are both meson AND EW contributions included?
2. If yes, how are they combined? (Additive? Do they use consistent parent PDG codes?)
3. If no, which production mode dominates at each mass point?

**Evidence from code:** `run_serial.py:117` skips `_fromTau` files but doesn't explicitly combine meson + EW:

```python
if match and "_fromTau" not in f.name:  # Skip fromTau to avoid double counting
    mass_str = match.group(1)
    mass_val = float(mass_str.replace('p', '.'))
    files.append((mass_val, mass_str, f))
```

**Physics concern:** At 5 GeV, B-meson production (œÉ ~ 5√ó10‚Å∏ pb) might compete with W production (œÉ ~ 2√ó10‚Å∏ pb). If both are generated but only one is analyzed, the limits could be significantly wrong.

**Recommendation:**
- Either document that only ONE regime is used per mass point (and specify which)
- OR implement explicit meson+EW combination logic with parent PDG filtering
- Add validation test comparing 5 GeV results with/without both contributions

---

### 1.2 ‚úÖ Adaptive Mass Spacing

**Verdict:** Good physics-motivated design

The grids are denser near kinematic thresholds:
- Kaon: m = 0.494 GeV ‚Üí dense sampling at 0.2-0.5 GeV
- D meson: m = 1.87 GeV ‚Üí dense sampling at 1.5-2.0 GeV
- Tau: m = 1.777 GeV ‚Üí dense sampling at 1.6-1.8 GeV

This is appropriate for capturing rapid BR variations near thresholds.

---

## 2. Cross-Section Normalization

### 2.1 ‚ö†Ô∏è CRITICAL: W/Z Branching Ratios Are Approximate

**File:** `analysis_pbc/models/hnl_model_hnlcalc.py:245-278`

**Code:**
```python
# W¬± ‚Üí ‚Ñì¬± N (kinematically allowed if m_N < m_W)
if mass < m_W:
    # Phase space suppression: (1 - m_N¬≤/m_W¬≤)¬≤
    phase_space_W = (1.0 - (mass / m_W)**2)**2
    # BR(W ‚Üí ‚ÑìN) ‚âà |U_‚Ñì|¬≤ √ó phase_space
    # Sum over all active lepton flavors
    br_W = (self.Ue2 + self.Umu2 + self.Utau2) * phase_space_W
    br_per_parent[24] = br_W
```

**Problems:**

1. **Missing W‚Üí‚ÑìŒΩ normalization:** The formula `BR(W‚Üí‚ÑìN) = |U_‚Ñì|¬≤ √ó phase_space` assumes W‚Üí‚ÑìN competes with vacuum, not with SM W‚Üí‚ÑìŒΩ. Should be:
   ```
   BR(W‚Üí‚ÑìN) = [Œì(W‚Üí‚ÑìN) / Œì_tot]
            ‚âà [|U_‚Ñì|¬≤ √ó Œì(W‚Üí‚ÑìŒΩ) √ó f_PS] / Œì_W
   ```
   where `Œì(W‚Üí‚ÑìŒΩ) ‚âà 0.33 Œì_W` for each lepton flavor.

2. **Phase space formula incomplete:** Should include helicity suppression and proper kinematics. For V-A coupling:
   ```
   Œì(W ‚Üí ‚Ñì N) ‚àù |U_‚Ñì|¬≤ √ó m_W¬≥ √ó (1 - m_N¬≤/m_W¬≤)¬≤ √ó (1 + m_N¬≤/m_W¬≤) / (192œÄ)
   ```
   The `(1 + m_N¬≤/m_W¬≤)` factor is missing.

3. **Factor of 0.5 for Z is ad-hoc:** Comment says "Factor of 1/2 relative to W" but this isn't justified. Z‚ÜíŒΩN has different couplings than W‚Üí‚ÑìN.

**Impact:** For m_N = 50 GeV (near m_W), this approximation could be off by factor of 2-3.

**References:**
- See arXiv:1805.08567 (Atre et al.) Eq. 2.11-2.12 for correct formulas
- HNLCalc should have these implemented for meson channels - why not for W/Z?

**Recommendation:**
- Use proper Œì(W‚Üí‚ÑìN)/Œì_W formula with helicity factors
- OR delegate to HNLCalc if it supports W/Z channels
- OR clearly document this as "approximate, good to factor ~2"

---

### 2.2 ‚ö†Ô∏è CRITICAL: Potential Double-Counting Between Pythia and MadGraph

**Files:**
- `production/pythia_production/main_hnl_production.cc:269-279` (B+ ‚Üí ‚Ñì+ N)
- `production/madgraph_production/scripts/run_hnl_scan.py` (W¬± ‚Üí ‚Ñì¬± N)

**Issue:** In the 5-80 GeV range, HNLs can be produced from:
1. **B-meson decays:** B ‚Üí ‚Ñì N (Pythia, "beauty" regime)
2. **W-boson decays:** W ‚Üí ‚Ñì N (MadGraph, "ew" regime)

Both processes contribute O(|U_‚Ñì|¬≤) and have comparable cross-sections at m ~ 5-10 GeV:
- œÉ(B‚Å∞) √ó BR(B‚Üí‚ÑìN) ~ 5√ó10‚Å∏ pb √ó 10‚Åª‚Å∂ |U|¬≤ = 5√ó10¬≤ |U|¬≤ pb
- œÉ(W) √ó BR(W‚Üí‚ÑìN) ~ 2√ó10‚Å∏ pb √ó 10‚Åª¬≥ |U|¬≤ ‚âà 2√ó10‚Åµ |U|¬≤ pb

So W dominates... but are they both included?

**Evidence from `run_serial.py:106-122`:**
```python
pattern = re.compile(rf"HNL_([0-9]+p[0-9]+)GeV_{flavour}_(kaon|charm|beauty|ew)(?:_direct|_fromTau)?\.csv")

files = []
for f in SIM_DIR.glob(f"*{flavour}*.csv"):
    if f.stat().st_size < 1000:  # Skip empty files
        print(f"[SKIP] Empty file: {f.name}")
        continue

    match = pattern.search(f.name)
    if match and "_fromTau" not in f.name:  # Skip fromTau to avoid double counting
        mass_str = match.group(1)
        mass_val = float(mass_str.replace('p', '.'))
        files.append((mass_val, mass_str, f))
```

**This code will include BOTH `HNL_5p0GeV_muon_beauty.csv` AND `HNL_5p0GeV_muon_ew.csv` if both exist!**

Then in `u2_limit_calculator.py:190-216`:
```python
for pid in unique_parents:
    BR_parent = br_per_parent.get(int(pid), 0.0)
    # ... computes N_sig contribution ...
    total_expected += lumi_fb * (sigma_parent_pb * 1e3) * BR_parent * eff_parent
```

**If the 5 GeV EW file has parent_pdg=24 (W) and the beauty file has parent_pdg=521 (B+), they'll be summed correctly.**

**BUT:** Are the kinematics identical? B‚Üí‚ÑìN produces HNLs with different Œ∑/œÜ distribution than pp‚ÜíW‚Üí‚ÑìN. If Pythia forces B‚Üí‚ÑìN, it doesn't know about W‚Üí‚ÑìN angular distributions.

**Questions:**
1. At m=5 GeV, are BOTH beauty.csv and ew.csv files generated?
2. If yes, are their parent PDG codes non-overlapping (e.g., beauty has 511/521/531, ew has 24/23)?
3. Is the sum physically correct, or does it double-count some production mechanism?

**Recommendation:**
- Document explicitly which mass points use which production mode
- Verify no PDG code collisions between meson and EW files at same mass
- Ideally: add diagnostic output showing parent composition for transition region

---

### 2.3 ‚ö†Ô∏è Missing K-Factors for Meson Production

**File:** `config/production_xsecs.py`

**Code:**
```python
SIGMA_CCBAR_PB = 24.0 * 1e9  # 24 mb = 2.4 √ó 10^10 pb
SIGMA_BBBAR_PB = 500.0 * 1e6  # 500 Œºb = 5.0 √ó 10^8 pb
```

**Issue:** These are Leading Order (LO) cross-sections from Pythia. In the MadGraph script (`run_hnl_scan.py:75`), there's a K-factor:

```python
K_FACTOR = 1.3
```

But this is **only applied to EW production**, not to meson production!

**Physics:** NLO QCD corrections to heavy quark production are substantial:
- œÉ(ccbar)_NLO / œÉ(ccbar)_LO ‚âà 1.5-2.0 (depending on PDF, scales)
- œÉ(bbbar)_NLO / œÉ(bbbar)_LO ‚âà 1.3-1.5

**Current implementation:** Meson limits use LO œÉ, while EW limits use œÉ √ó 1.3. This creates a **systematic discontinuity** at the meson‚ÜíEW transition.

**Recommendation:**
- Apply K-factors to meson cross-sections: `SIGMA_CCBAR_PB_NLO = 24.0 * 1e9 * 1.5`
- Document the chosen K-factors and their uncertainties
- OR use NNLO cross-sections from literature (e.g., arXiv:1610.07922 for heavy quarks)

---

### 2.4 ‚úÖ Fragmentation Fractions Are Reasonable

**Verdict:** Good

```python
FRAG_C_D0     = 0.59  # D0 / D0bar
FRAG_C_DPLUS  = 0.24  # D+ / D-
FRAG_C_DS     = 0.10  # Ds+ / Ds-

FRAG_B_B0     = 0.40  # B0 / B0bar
FRAG_B_BPLUS  = 0.40  # B+ / B-
FRAG_B_BS     = 0.10  # Bs0 / Bs0bar
```

These match PDG 2024 values within ~10%. The factor-of-2 for particle+antiparticle is correctly applied in `get_parent_sigma_pb()`.

**Minor note:** Fragmentation fractions have ~5-10% uncertainties that should propagate to final limits (currently not included).

---

## 3. Pythia Production

### 3.1 ‚úÖ Forced Decay Strategy is Correct

**File:** `production/pythia_production/main_hnl_production.cc`

The approach of forcing BR=1.0 for kinematic sampling is standard and correct:

```cpp
// Lines 234-239
pythia.readString("321:onMode = off");
pythia.readString("321:addChannel = 1 1.0 0 " + lepBar + " " + hnl);
```

The comment at lines 20-54 correctly explains the division of labor:
- **Pythia:** Generates kinematics with forced decays
- **HNLCalc:** Provides real BR(parent‚Üí‚ÑìN, |U|¬≤) in analysis stage
- **No double-counting:** Weights are relative, cross-sections external

This matches MATHUSLA/CODEX-b methodology.

---

### 3.2 ‚ö†Ô∏è Semileptonic Decays Use Phase Space, Not Matrix Elements

**File:** `main_hnl_production.cc:293-367`

**Code:**
```cpp
// D0 -> K- ‚Ñì+ N (semileptonic)
// Note: Using meMode=0 (phase space) for simplicity
pythia.readString("421:addChannel = 1 1.0 0 -321 " + lepBar + " " + hnl);
```

**Issue:** For 3-body semileptonic decays (e.g., D‚Å∞ ‚Üí K‚Åª ‚Ñì‚Å∫ N), the code uses Pythia's default `meMode=0`, which is **uniform phase space**.

**Physics:** Real D‚ÜíK‚ÑìŒΩ decays have form-factor suppression and angular distributions (see arXiv:hep-ph/0512066). The HNL case D‚ÜíK‚ÑìN should have similar kinematics.

**Impact:**
- HNL momentum spectrum may be slightly wrong
- Geometric acceptance depends on Œ∑/œÜ distribution
- For sensitivity estimates, phase space is probably OK (factor ~1.5 uncertainty)
- For publication-quality results, should use proper matrix elements

**Evidence this might be OK:** VALIDATION.md claims "validated by arXiv:2103.11494 as adequate for sensitivity estimates" but doesn't cite specific validation plots.

**Recommendation:**
- Quantify difference between phase-space and form-factor kinematics (e.g., compare Œ∑ distributions)
- If geometric acceptance changes by >20%, implement proper matrix elements
- OR clearly document this as "order-of-magnitude sensitivity, not precision limit"

---

### 3.3 ‚ö†Ô∏è Antiparticle Channels May Be Incomplete

**File:** `main_hnl_production.cc:300-311`

**Code for D‚Å∞:**
```cpp
pythia.readString("421:onMode = off");
pythia.readString("421:addChannel = 1 1.0 0 -321 " + lepBar + " " + hnl);
pythia.readString("-421:onMode = off");
pythia.readString("-421:addChannel = 1 1.0 0 321 " + lep + " " + hnl);
```

This correctly implements D‚Å∞ ‚Üí K‚Åª ‚Ñì‚Å∫ N and DÃÑ‚Å∞ ‚Üí K‚Å∫ ‚Ñì‚Åª NÃÑ.

**But for charged mesons:**
```cpp
// D+ -> K0bar ‚Ñì+ N (semileptonic, K0bar = -311)
pythia.readString("411:addChannel = 1 0.5 0 -311 " + lepBar + " " + hnl);
pythia.readString("-411:addChannel = 1 0.5 0 311 " + lep + " " + hnl);
```

**Issue:** The channel weight is **0.5**, presumably because the 2-body leptonic D‚Å∫‚Üí‚Ñì‚Å∫N was already added with weight 1.0. This means:
- 50% of D‚Å∫ decays go to ‚Ñì‚Å∫N (2-body)
- 50% of D‚Å∫ decays go to KÃÑ‚Å∞‚Ñì‚Å∫N (3-body)

**Physics question:** Is this the correct mix for modeling the INCLUSIVE D‚Å∫‚Üí(anything) + ‚Ñì‚Å∫N process?

In reality:
- BR(D‚Å∫‚Üí‚Ñì‚Å∫ŒΩ) ~ 10‚Åª¬≥ (helicity suppressed)
- BR(D‚Å∫‚ÜíKÃÑ‚Å∞‚Ñì‚Å∫ŒΩ) ~ 10‚Åª¬≤ (dominant leptonic mode)

So the 3-body should dominate, not 50/50. The code may be generating too many 2-body events.

**However:** HNLCalc will apply the correct inclusive BR in the analysis, so the 50/50 split only affects kinematics, not normalization. As long as the Œ∑/œÜ distributions are reasonable, this is probably OK for sensitivity estimates.

**Recommendation:**
- Validate that the final Œ∑/œÜ distributions match expectations (e.g., from full B‚ÜíX‚ÑìŒΩ simulations)
- Consider using BR ratios from PDG to weight channels more accurately

---

### 3.4 ‚úÖ Tau Production Modes Are Well-Designed

**Verdict:** Excellent

The separation of tau production into `direct` and `fromTau` modes (lines 390-438) correctly handles the two O(U_tau¬≤) contributions:

- **MODE A (direct):** B/Ds ‚Üí œÑ N (mixing at meson vertex)
- **MODE B (fromTau):** B/Ds ‚Üí œÑ ŒΩ, then œÑ ‚Üí œÄ N (mixing at tau decay)

These are independent and should be added. The code explicitly avoids double-counting by generating separate samples. Good!

**Minor note:** `run_serial.py:117` has logic to skip `_fromTau` files:
```python
if match and "_fromTau" not in f.name:  # Skip fromTau to avoid double counting
```

**Question:** Should `_fromTau` files be analyzed separately and added, or are they truly duplicates? The comment suggests they're skipped to avoid double-counting, but the physics says they're independent contributions.

**Recommendation:**
- Clarify in documentation whether `_fromTau` files should be analyzed
- If they should be included, modify the file matching logic
- If they shouldn't, explain why (e.g., "negligible compared to direct" or "kinematically similar so included in direct")

---

## 4. HNLCalc Integration

### 4.1 ‚úÖ Wrapper Design is Clean

**File:** `models/hnl_model_hnlcalc.py`

The `HNLModel` class provides a nice interface:
```python
model = HNLModel(mass_GeV=2.6, Ue2=0.0, Umu2=1e-6, Utau2=0.0)
ctau0_m = model.ctau0_m              # Proper lifetime [m]
brs = model.production_brs()         # {parent_pdg: BR(parent‚Üí‚ÑìN)}
```

The coupling normalization (lines 112-138) correctly handles the HNLCalc convention of passing `sqrt(U¬≤)` ratios.

---

### 4.2 ‚ö†Ô∏è SECURITY: Use of `eval()` is Dangerous

**File:** `models/hnl_model_hnlcalc.py:206-209`

**Code:**
```python
br_string = channel['br']    # String like "hnl.get_2body_br(411, -11)"

# Evaluate the BR string
br_formula = eval(br_string)
# Now evaluate that formula
br_value = eval(br_formula)
```

**Problem:** **DOUBLE `eval()` on untrusted input!**

If the HNLCalc database is ever compromised or malformed, an attacker could execute arbitrary Python code:
```python
# Malicious BR string:
channel['br'] = "__import__('os').system('rm -rf /')"
```

**This is a critical security vulnerability.**

**Mitigation (current):** HNLCalc is a trusted repository from collaborators, so the risk is low for scientific use. But if this code is ever run on untrusted data...

**Recommendation:**
- Use `ast.literal_eval()` for constants
- OR use HNLCalc's internal methods without string evaluation
- OR sandbox the evaluation with `restricted_exec` or similar
- Add input validation to check `br_string` matches expected format

---

### 4.3 ‚ö†Ô∏è Channel Coverage Not Validated

**File:** `models/hnl_model_hnlcalc.py:183-243`

**Code:**
```python
# Get all 2-body and 3-body production channels from HNLCalc (mesons only)
channels_2body = sum(self._hnlcalc.get_channels_2body()["mode"].values(), [])
channels_3body = sum(self._hnlcalc.get_channels_3body()["mode"].values(), [])
```

**Question:** How do we know HNLCalc includes ALL the channels we generated in Pythia?

From Pythia production:
- K¬± ‚Üí ‚Ñì N
- D‚Å∞ ‚Üí K ‚Ñì N
- D¬± ‚Üí ‚Ñì N, K‚Å∞ ‚Ñì N
- Ds¬± ‚Üí ‚Ñì N
- B‚Å∞ ‚Üí D ‚Ñì N
- B¬± ‚Üí ‚Ñì N, D‚Å∞ ‚Ñì N
- Bs ‚Üí Ds ‚Ñì N
- Œõb ‚Üí Œõc ‚Ñì N

Does HNLCalc's "150+ production modes" include all of these for all lepton flavors?

**Evidence from VALIDATION.md:**
```
[WARN] Mass 2.60 GeV: 1 parent PDG(s) have no HNLCalc BR: [310]
‚Üí Discarding events (silent data loss)
```

So there ARE missing channels (KS‚Å∞ = PDG 310). Are there others?

**Recommendation:**
- Run `limits/diagnostic_pdg_coverage.py` for all mass points
- Document which parent PDGs are not covered by HNLCalc
- Estimate the fractional loss (VALIDATION.md claims <0.1% for KS‚Å∞, but what about others?)
- Consider adding manual BRs for missing channels if they're important

---

### 4.4 ‚úÖ Lifetime Scaling is Correct

**File:** `models/hnl_model_hnlcalc.py:144-155`

The lifetime calculation correctly passes `coupling = sqrt(Ue2 + Umu2 + Utau2)` to HNLCalc, and HNLCalc returns `ctau[0]` in meters. The scaling `cœÑ ‚àù 1/|U|¬≤` is properly handled inside HNLCalc.

**Validated:** VALIDATION.md confirms `cœÑ(|U|¬≤ = 10‚Åª‚Å∑) = cœÑ‚ÇÄ / |U|¬≤ √ó 10‚Åª‚Å∂ = 5.748√ó10¬≥ m ‚úì`

---

## 5. Geometry and Ray-Tracing

### 5.1 ‚úÖ Mesh Construction is Correct

**File:** `geometry/per_parent_efficiency.py:119-217`

The drainage gallery tube mesh:
- Uses corrected vertex coordinates from Higgs‚ÜíLLP script
- Applies proper coordinate transformations (shift and scale)
- Sets z = 22m above IP
- Radius = 1.54m (1.4m √ó 1.1 safety factor)

The tube is built with `n_segments=32` for smoothness. The `trimesh` library is a standard tool for 3D mesh operations.

**Minor detail:** The code inverts the mesh if `volume < 0` (line 205-207), which is good defensive programming for ensuring correct normal directions.

---

### 5.2 ‚ö†Ô∏è Ray-Tracing Edge Cases

**File:** `geometry/per_parent_efficiency.py:310-372`

**Good practices:**
- Skips non-finite Œ∑/œÜ (lines 326-328)
- Guards against zero-length directions (lines 333-335)
- Catches `RTreeError` from invalid ray directions (lines 337-346)
- Tracks how many events are skipped

**Issue:** For rays that hit the tube, the code assumes:
```python
if len(locations) >= 2:
    df.at[idx, "hits_tube"] = True
```

**Question:** What if `len(locations) > 2` (ray grazes the tube tangentially or tube has self-intersections)?

The code sorts distances and takes `distances[0]` (entry) and `distances[1]` (exit), but for a curved tube, a ray might intersect multiple times (e.g., enter, exit, enter again, exit again).

**Likely OK because:** The tube is mostly convex and the mesh is well-constructed. Self-intersections would be a mesh topology error.

**Recommendation:**
- Add assertion: `assert len(locations) == 2 or len(locations) == 0` to catch mesh errors
- OR handle multiple intersections by pairing them (entry‚ÇÅ, exit‚ÇÅ, entry‚ÇÇ, exit‚ÇÇ, ...) and summing path lengths

---

### 5.3 ‚úÖ Boost Factor Calculation

**File:** `geometry/per_parent_efficiency.py:302-307`

```python
if "boost_gamma" in df.columns:
    df["beta_gamma"] = df["boost_gamma"]
else:
    df["beta_gamma"] = df["momentum"] / df["mass"]
```

This correctly computes `Œ≤ Œ≥ = p/m` for the Lorentz boost. The decay probability formula (in `u2_limit_calculator.py:166-179`) uses:

```python
lam = beta_gamma * ctau0_m
P_decay = np.exp(-entry/lam) * (-np.expm1(-length/lam))
```

This is the correct formula:
```
P_decay = exp(-d_entry/Œª) √ó [1 - exp(-d_path/Œª)]
```
where `Œª = Œ≤ Œ≥ cœÑ‚ÇÄ` is the boosted decay length.

**Numerically stable:** Using `expm1(x) = exp(x) - 1` for small x is good practice.

---

## 6. Signal Calculation

### 6.1 ‚úÖ Per-Parent Counting is Correct

**File:** `limits/u2_limit_calculator.py:70-231`

The methodology is well-documented and correctly implemented:

```python
for pid in unique_parents:
    BR_parent = br_per_parent.get(int(pid), 0.0)
    sigma_parent_pb = get_parent_sigma_pb(int(pid))

    mask_parent = np.abs(parent_id) == pid
    w = weights[mask_parent]
    P = P_decay[mask_parent]

    eff_parent = np.sum(w * P) / w_sum
    total_expected += lumi_fb * (sigma_parent_pb * 1e3) * BR_parent * eff_parent
```

**This implements:**
```
N_sig = Œ£_parents [ L √ó œÉ_parent √ó BR(parent‚Üí‚ÑìN) √ó Œµ_geom(parent) ]
```

which is the standard LLP detector formula (MATHUSLA, ANUBIS, CODEX-b).

**Key insight:** Each HNL is weighted by its parent's cross-section, avoiding the per-event probability product `P_event = 1 - ‚àè(1-P_i)` which would lose ~50% sensitivity. Excellent!

---

### 6.2 ‚úÖ Weight Handling is Correct

**File:** `u2_limit_calculator.py:332-350`

The code includes defensive checks for weights:

```python
if w_max > 1e6:
    print(f"[ERROR] Weights suspiciously large (max={w_max:.2e})!")
    print(f"        This looks like absolute cross-section (pb), not relative MC weight.")
    return None
elif w_max > 1000:
    print(f"[WARN] Weights unusually large (max={w_max:.2e}, mean={w_mean:.2e})")
```

**Good!** This catches the common mistake of using `pythia.info.sigmaGen()` instead of `pythia.info.weight()`.

**Verified:** `main_hnl_production.cc:654` correctly uses:
```cpp
double weight = pythia.info.weight();
```

---

### 6.3 ‚ö†Ô∏è NaN Handling Could Be More Transparent

**File:** `u2_limit_calculator.py:149-163`

```python
mask_valid = np.isfinite(parent_id)
if not np.all(mask_valid):
    # We silently drop them here to keep the scan fast;
    # logging happens in the worker function usually.
    parent_id = parent_id[mask_valid]
    # ... filter all arrays ...
```

**Comment says "silently drop them"** but then claims "logging happens in the worker function."

**Question:** Where exactly is the logging? In `_scan_single_mass()` at lines 310-331, NaNs are dropped with:
```python
print(f"[INFO] m={mass_str} {flavour}: Dropping {n_dropped} rows with NaNs...")
```

But this message might not be visible if running in parallel (`ProcessPoolExecutor`). Workers log to stdout, which might be buffered or lost.

**Recommendation:**
- Log to a file, not just stdout, for worker processes
- OR aggregate dropped-event statistics and report at the end
- Make sure the final summary includes "X events dropped due to NaNs/missing PDGs"

---

## 7. MadGraph EW Production

### 7.1 ‚úÖ Process Generation is Well-Automated

**File:** `production/madgraph_production/scripts/run_hnl_scan.py`

The three-step workflow (generate process ‚Üí write cards ‚Üí run events ‚Üí convert LHE‚ÜíCSV) is clean and well-documented.

**Good practices:**
- Uses central mass grid from `config_mass_grid.py`
- Handles Docker paths correctly
- Extracts cross-sections from banner files
- Appends summary CSV with metadata

---

### 7.2 ‚ö†Ô∏è LHE‚ÜíCSV Conversion Not Shown

**File:** `run_hnl_scan.py:440-473`

```python
from lhe_to_csv import LHEParser

parser = LHEParser(lhe_file, mass, flavour)
n_events = parser.write_csv(csv_output)
```

**Issue:** The `LHEParser` class is not shown in the files I reviewed.

**Questions:**
1. Does it correctly extract `parent_pdg` for W/Z bosons (PDG 24/23)?
2. Does it compute the correct boost factor `beta_gamma = p/m`?
3. Does it handle gzipped LHE files (`.lhe.gz`)?
4. Does it parse the LHE weight correctly?

**Recommendation:**
- Review `scripts/lhe_to_csv.py` to ensure it matches the Pythia CSV format
- Validate that a few EW CSV files have correct columns and PDG codes

---

### 7.3 ‚ö†Ô∏è K-Factor Applied Only to Summary, Not Analysis

**File:** `run_hnl_scan.py:74-75, 491-507`

```python
K_FACTOR = 1.3

# Later, in summary CSV:
row = (
    f"{mass:.1f},"
    f"{flavour},"
    f"{xsec_data['xsec_pb']:.6e},"
    f"{xsec_data['xsec_error_pb']:.6e},"
    f"{K_FACTOR:.2f},"  # <-- K-factor stored but not applied
    f"{n_events_csv},"
    f"{csv_rel_path},"
    f"{timestamp}"
)
```

**The K-factor is recorded but not used!**

**In the analysis:** `config/production_xsecs.py` has:
```python
SIGMA_W_PB = 2.0 * 1e8  # œÉ(pp‚ÜíW) ~ 200 nb (W+ + W- combined)
```

This is presumably an LO cross-section. If the MadGraph simulation also uses LO, then we should apply `K_FACTOR=1.3` to `SIGMA_W_PB` in the analysis.

**But:** If MadGraph runs at NLO (check the `param_card`), then the cross-section from the banner already includes NLO corrections, and we shouldn't multiply by 1.3 again.

**Recommendation:**
- Check whether MadGraph runs at LO or NLO (look at the `param_card_template.dat`)
- If LO: Apply K-factor to `SIGMA_W_PB` and `SIGMA_Z_PB` in `production_xsecs.py`
- If NLO: Use the cross-section from the MadGraph banner directly (read from summary CSV)
- Document which is being used

---

## 8. Statistical Treatment

### 8.1 ‚ö†Ô∏è CRITICAL: No Systematic Uncertainties

**File:** `limits/u2_limit_calculator.py:242-267`

The limit is defined as:
```python
mask = Nsig >= N_limit
```
where `N_limit = 3.0`.

**Issue:** This assumes:
1. Zero background
2. Perfect detector efficiency
3. Exact cross-sections
4. No uncertainty on HNLCalc BRs

**Reality:**
- œÉ(ccbar) uncertainty: ~20% (PDF + scale variation)
- œÉ(bbbar) uncertainty: ~15%
- Fragmentation fractions: ~10%
- HNLCalc BRs: Unknown (probably ~20-50% from form factors, PDFs)
- Geometric acceptance: ~5-10% (mesh precision, coordinate uncertainty)

**Impact:** Combined systematic ~30-50%. For N_limit=3, this could shift limits by factor ~1.5-2 in |U|¬≤.

**Recommendation:**
- Implement systematic uncertainties as scale factors on N_sig
- Use profile likelihood or Bayesian marginalization
- OR clearly state "statistical limits only, systematic uncertainties O(50%) not included"
- For publication: Add bands showing ¬±1œÉ systematic variation

---

### 8.2 ‚ö†Ô∏è N‚â•3 Threshold is Oversimplified

**File:** `limits/run_serial.py:71-83`

```python
mask_excluded = (N_scan >= 3.0)
```

**Issue:** Poisson statistics for N_obs=0 (expected background) gives different thresholds:

| Confidence Level | N_sig threshold |
|------------------|----------------|
| 90% CL | 2.30 |
| 95% CL | 2.99 |
| 3œÉ (99.7% CL) | 4.75 |

So `N>=3` is somewhere between 95% CL and 3œÉ, but not exactly either.

**Standard practice:** Use 90% CL ‚Üí N=2.3 or 95% CL ‚Üí N=3.0.

**Current implementation claims "90% CL" (VALIDATION.md line 19) but uses N=3.0, which is 95% CL.**

**Recommendation:**
- For 90% CL, use `N_limit = 2.30`
- For 95% CL, keep `N_limit = 3.00`
- Document which is being used and cite Poisson statistics reference (e.g., PDG Statistics Review)

---

### 8.3 ‚ö†Ô∏è Island Boundary Detection is Coarse

**File:** `run_serial.py:85-90`

```python
indices_excl = np.where(mask_excluded)[0]
eps2_min = eps2_grid[indices_excl[0]]
eps2_max = eps2_grid[indices_excl[-1]]
```

**Issue:** With 100 log-spaced points from 10‚Åª¬π¬≤ to 10‚Åª¬≤, the grid spacing is:

```
Œî log‚ÇÅ‚ÇÄ(Œµ¬≤) = (log‚ÇÅ‚ÇÄ(10‚Åª¬≤) - log‚ÇÅ‚ÇÄ(10‚Åª¬π¬≤)) / 100 = 10/100 = 0.1 decades
```

So the island boundaries are uncertain by ~0.1 decades (factor of 1.26 in Œµ¬≤).

**For precision limits:** Should interpolate to find N_sig(Œµ¬≤) = 3.0 crossings.

**For sensitivity estimates:** Current precision is fine.

**Recommendation:**
- For publication: Interpolate (linear in log space) to find exact crossing
- For internal use: Current grid is adequate, but document the ~25% precision on boundaries

---

## 9. Code Quality and Robustness

### 9.1 ‚úÖ Defensive Programming

**Verdict:** Excellent

The code includes numerous safety checks:
- NaN filtering (multiple locations)
- Weight magnitude checks (`w_max > 1e6` ‚Üí error)
- Parent PDG coverage diagnostics
- Geometry cache validation
- Empty file skipping (`file_size < 1000` bytes)

**This is professional-quality defensive coding.**

---

### 9.2 ‚úÖ Documentation is Comprehensive

**Verdict:** Outstanding

The docstrings and comments are extremely detailed:
- `CLAUDE.md` provides excellent quick-start guide
- `VALIDATION.md` shows thorough methodology validation
- `u2_limit_calculator.py` has 100+ line docstring explaining per-parent counting
- `main_hnl_production.cc` has 65-line header explaining normalization strategy

**This is publication-ready documentation.**

---

### 9.3 ‚ö†Ô∏è Testing Coverage is Incomplete

**Files present:**
- `tests/test_pipeline.py` - Smoke tests
- `tests/test_26gev_muon.py` - Benchmark validation
- `tests/closure_anubis/` - ANUBIS comparison (not reviewed in detail)

**Missing tests:**
- Unit tests for HNLCalc wrapper (BR scaling, lifetime scaling)
- Unit tests for cross-section lookup (all PDG codes)
- Integration test for meson+EW combination at transition masses
- Regression test for island shape (should be stable as code evolves)

**Recommendation:**
- Add `pytest` test suite with:
  - `test_hnlcalc_lifetime_scaling()` - verify cœÑ ‚àù 1/|U|¬≤
  - `test_hnlcalc_br_scaling()` - verify BR ‚àù |U|¬≤
  - `test_xsec_all_pdg()` - check all parent PDGs have non-zero œÉ
  - `test_meson_ew_combination()` - verify 5 GeV has both contributions
  - `test_island_shape()` - compare to saved reference island

---

## 10. Physics Validation

### 10.1 ‚úÖ 2.6 GeV Muon Benchmark is Reasonable

**From VALIDATION.md:**
- Mass: 2.6 GeV
- Peak signal: 2880 events at |U_mu|¬≤ ~ 10‚Åª‚Å∂
- Exclusion: |U_mu|¬≤ ‚àà [6.9√ó10‚Åª‚Åπ, 2.4√ó10‚Åª‚Åµ]
- Island width: 3.54 decades
- Parent composition: 86% B-mesons

**Physics checks:**
1. **Is 2880 events plausible?**
   - L = 3000 fb‚Åª¬π, œÉ(B‚Å∞) = 4√ó10‚Å∏ pb, BR ~ 10‚Åª‚Å∑ ‚Üí N ~ 3000 fb‚Åª¬π √ó 4√ó10‚Å∏ pb √ó 10‚Åª‚Å∑ √ó 1% (geom) ~ 1200 events
   - Factor 2.4 larger ‚Üí includes B‚Å∫, Bs, Œõb contributions
   - ‚úÖ Reasonable

2. **Is island width sensible?**
   - Detector at z=22m, need cœÑ ~ 10-100m for optimal sensitivity
   - At |U|¬≤=10‚Åª‚Å∂: cœÑ ~ 100m ‚úì
   - At |U|¬≤=10‚Åª‚Åπ: cœÑ ~ 100 km (too long) ‚úì
   - At |U|¬≤=10‚Åª‚Åµ: cœÑ ~ 10m (marginal) ‚úì
   - ‚úÖ Island structure is physically sensible

3. **Is geometric acceptance correct?**
   - 1.41% of HNLs hit detector
   - Solid angle of tube at 22m distance ~ few % √ó boost collimation
   - ‚úÖ Reasonable

**Conclusion:** Benchmark passes sanity checks.

---

### 10.2 ‚ö†Ô∏è No Comparison to Other Experiments

**Missing:**
- Comparison to MATHUSLA projections (arXiv:1811.00927)
- Comparison to ANUBIS projections (arXiv:1909.13022)
- Comparison to CODEX-b limits (arXiv:2103.03281)

**Recommendation:**
- Plot CMS drainage gallery limits alongside other experiments
- Validate that the sensitivity is in the expected ballpark (should be similar to CODEX-b due to similar geometry)
- If significantly different, investigate why

---

### 10.3 ‚ö†Ô∏è ANUBIS Closure Test Not Fully Documented

**File:** `tests/closure_anubis/` exists but I didn't review it in detail.

**From VALIDATION.md:** "ANUBIS closure" is mentioned, suggesting the code reproduces ANUBIS results.

**Recommendation:**
- Document the ANUBIS closure test results clearly
- Show that key numbers match (e.g., 2.6 GeV muon limit agrees within X%)
- If there are differences, explain them (e.g., different detector geometry, different HNL model)

---

## 11. Recommendations

### 11.1 Critical Fixes (Before Publication)

1. **Fix W/Z BR formula** (Section 2.1)
   - Use proper Œì(W‚Üí‚ÑìN)/Œì_W with helicity factors
   - Cite arXiv:1805.08567 or similar
   - Validate against known results for m_N << m_W

2. **Resolve meson-EW double-counting** (Section 2.2)
   - Document which production mode is used at each mass
   - Verify parent PDG codes don't overlap
   - If both are used, validate that sum is correct

3. **Implement systematic uncertainties** (Section 8.1)
   - At minimum: ¬±30% envelope on N_sig
   - Ideally: Profile likelihood with individual œÉ, BR, eff uncertainties
   - Document what's included and what's not

4. **Fix N_limit to match claimed CL** (Section 8.2)
   - 90% CL ‚Üí N=2.30
   - 95% CL ‚Üí N=3.00
   - Document which is used

5. **Remove `eval()` security vulnerability** (Section 4.2)
   - Use `ast.literal_eval()` or direct method calls
   - Add input validation

---

### 11.2 Important Improvements (Before Wide Release)

6. **Apply K-factors to meson production** (Section 2.3)
   - Use NLO cross-sections for ccbar, bbbar
   - Document K-factors and their uncertainties

7. **Validate semileptonic kinematics** (Section 3.2)
   - Compare phase-space vs. form-factor Œ∑ distributions
   - Quantify impact on geometric acceptance

8. **Clarify tau `_fromTau` handling** (Section 3.4)
   - Document whether these files should be included
   - If yes, modify file matching logic
   - If no, explain why they're excluded

9. **Add unit tests** (Section 9.3)
   - HNLCalc scaling tests
   - Cross-section lookup tests
   - Meson+EW combination tests

10. **Compare to other experiments** (Section 10.2)
    - Plot vs. MATHUSLA, ANUBIS, CODEX-b
    - Validate sensitivity is reasonable

---

### 11.3 Minor Enhancements (Nice to Have)

11. **Interpolate island boundaries** (Section 8.3)
    - Find exact N_sig=3 crossings
    - Improves precision from ~25% to ~1%

12. **Better ray-tracing edge case handling** (Section 5.2)
    - Assert `len(locations) in [0, 2]`
    - OR handle multiple intersections

13. **Improve NaN logging** (Section 6.3)
    - Log dropped events to file
    - Report summary statistics

14. **Document ANUBIS closure** (Section 10.3)
    - Show explicit comparison numbers
    - Explain any differences

---

## 12. Conclusion

This is a **solid, well-designed analysis** with excellent documentation and defensive programming. The core methodology (per-parent counting, HNLCalc integration, geometry ray-tracing) is correct and follows established LLP detector standards.

**However,** there are several issues that could affect the quantitative reliability of results:

### Show-Stoppers for Publication:
- W/Z BR formula is approximate (factor ~2 uncertainty)
- Meson-EW combination not clearly documented (potential double-counting)
- No systematic uncertainties (O(50%) missing)
- Statistical treatment oversimplified (N‚â•3 not exactly 90% CL)

### Show-Stoppers for Public Release:
- Security vulnerability (`eval()` on HNLCalc strings)
- Missing validation against other experiments

### Quality-of-Life Issues:
- K-factors not applied to meson production
- Semileptonic matrix elements are phase-space approximations
- Testing coverage incomplete

**Overall Grade: B+ (Very Good, but needs fixes for publication)**

The authors have done an impressive job implementing a complex analysis pipeline. With the critical fixes listed above, this would be publication-ready. For internal sensitivity estimates, it's already quite good (with caveats documented).

---

## Appendix A: Recommended Reading

For the authors to cross-check their implementation:

1. **HNL Production BRs:**
   - arXiv:1805.08567 (Atre et al.) - Canonical HNL review, Eq. 2.11-2.12 for W/Z decays

2. **LLP Detector Methodology:**
   - arXiv:1811.00927 (MATHUSLA) - Per-parent counting, geometry calculations
   - arXiv:1909.13022 (ANUBIS) - Similar methodology, good for cross-checks
   - arXiv:2103.03281 (CODEX-b) - CMS-specific LLP detector, closest comparison

3. **HNL Physics:**
   - arXiv:2405.07330 (HNLCalc paper) - Validate your usage matches their examples
   - arXiv:2103.11494 (Pythia HNL validation) - Cited in your code for semileptonic kinematics

4. **Statistics:**
   - PDG Statistics Review - Poisson limits, systematic uncertainties
   - arXiv:1007.1727 (Cowan et al.) - Profile likelihood for systematics

5. **Heavy Quark Production:**
   - arXiv:1610.07922 - NNLO bbbar cross-sections
   - arXiv:1707.07305 - FONLL charm production

---

## Appendix B: Files Reviewed

**Core Analysis:**
- `config_mass_grid.py` (209 lines)
- `analysis_pbc/limits/run_serial.py` (163 lines)
- `analysis_pbc/limits/u2_limit_calculator.py` (529 lines)
- `analysis_pbc/config/production_xsecs.py` (204 lines)
- `analysis_pbc/geometry/per_parent_efficiency.py` (373 lines)
- `analysis_pbc/models/hnl_model_hnlcalc.py` (289 lines)

**Production:**
- `production/pythia_production/main_hnl_production.cc` (724 lines)
- `production/madgraph_production/scripts/run_hnl_scan.py` (678 lines)

**Testing & Documentation:**
- `analysis_pbc/tests/test_26gev_muon.py` (118 lines)
- `analysis_pbc/VALIDATION.md` (368 lines)
- `analysis_pbc/README.md` (234 lines)
- `CLAUDE.md` (project overview)

**Total:** ~3900 lines reviewed

---

*End of Review*
